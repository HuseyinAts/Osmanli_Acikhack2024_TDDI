{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuseyinAts/Osmanli_Acikhack2024_TDDI/blob/main/Osmanlica_ResnetOcr_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1_adl%C4%B1_not_defterinin_kopyas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uToZxZCGrfDM",
        "outputId": "66577531-0604-414f-b7b6-c9c65727ee44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDxGLTUPzpg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import editdistance\n",
        "\n",
        "# OsmanlicaDataset sınıfını burada tanımlayın\n",
        "class OsmanlicaDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_file, transform=None, max_length=100):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.data = []\n",
        "        self.char_to_idx = {}\n",
        "        self.idx_to_char = {}\n",
        "\n",
        "        with open(label_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            image_name, label = line.strip().split(',')\n",
        "            self.data.append((image_name, label))\n",
        "\n",
        "            for char in label:\n",
        "                if char not in self.char_to_idx:\n",
        "                    idx = len(self.char_to_idx)\n",
        "                    self.char_to_idx[char] = idx\n",
        "                    self.idx_to_char[idx] = char\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name, label = self.data[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label_encoded = [self.char_to_idx.get(char, 0) for char in label[:self.max_length]]\n",
        "        label_encoded += [0] * (self.max_length - len(label_encoded))  # Padding\n",
        "\n",
        "        return image, torch.tensor(label_encoded, dtype=torch.long)\n",
        "\n",
        "# ResNetGRUAttention model sınıfını burada tanımlayın\n",
        "\n",
        "# FocalLoss sınıfını burada tanımlayın\n",
        "\n",
        "# Diğer yardımcı fonksiyonları tanımlayın (save_model, load_model, predict, vb.)\n",
        "\n",
        "def train(args):\n",
        "    # Eğitim kodunuz burada\n",
        "\n",
        "def main():\n",
        "    # Ana fonksiyon kodunuz burada\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "8T4Zg6gFQShg",
        "outputId": "5fb371e3-4da6-4c85-d573-9f999a87a423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 56 (<ipython-input-2-8e0d3803b874>, line 59)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-8e0d3803b874>\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    def main():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uAZWSbzMBxZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hakantrkgl/ottomantranslate/Dataset/chars\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9m5HJdqA9De",
        "outputId": "d6aa5c68-afe8-4951-f2b5-daf84cdc024d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chars'...\n",
            "remote: Not Found\n",
            "fatal: repository 'https://github.com/hakantrkgl/ottomantranslate/Dataset/chars/' not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # Bu satırı ekleyin\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import editdistance\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# ... (Önceki sınıf ve fonksiyon tanımlamaları aynı kalacak)\n",
        "# OsmanlicaDataset sınıfını burada tanımlayın\n",
        "class OsmanlicaDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_file, transform=None, max_length=100):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.data = []\n",
        "        self.char_to_idx = {}\n",
        "        self.idx_to_char = {}\n",
        "\n",
        "        with open(label_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split('\\t')  # Tab ile ayır\n",
        "            if len(parts) >= 2:\n",
        "                image_name = parts[0].split('.')[0]  # Dosya uzantısını kaldır\n",
        "                label = '\\t'.join(parts[1:])  # Etiketin içinde tab varsa\n",
        "                image_path = os.path.join(self.image_dir, f\"{image_name}.png\")\n",
        "                if os.path.exists(image_path):  # Dosyanın var olduğunu kontrol et\n",
        "                    self.data.append((image_name, label))\n",
        "\n",
        "                    for char in label:\n",
        "                        if char not in self.char_to_idx:\n",
        "                            idx = len(self.char_to_idx)\n",
        "                            self.char_to_idx[char] = idx\n",
        "                            self.idx_to_char[idx] = char\n",
        "                else:\n",
        "                    print(f\"Image file not found: {image_path}\")\n",
        "            else:\n",
        "                print(f\"Ignoring invalid line: {line.strip()}\")\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} valid image-label pairs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name, label = self.data[idx]\n",
        "        image_path = os.path.join(self.image_dir, f\"{image_name}.png\")\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label_encoded = [self.char_to_idx.get(char, 0) for char in label[:self.max_length]]\n",
        "        label_encoded += [0] * (self.max_length - len(label_encoded))  # Padding\n",
        "\n",
        "        return image, torch.tensor(label_encoded, dtype=torch.long)\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(model, path, device):\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def predict(model, image_path, transform, idx_to_char, device, max_length):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output, 2)\n",
        "\n",
        "    predicted = predicted.squeeze().cpu().numpy()\n",
        "    predicted_text = ''.join([idx_to_char[idx] for idx in predicted if idx in idx_to_char])\n",
        "    return predicted_text[:max_length]  # Maksimum uzunluğa göre kırpma\n",
        "\n",
        "def train(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    dataset = OsmanlicaDataset(args.image_dir, args.label_file, transform=transform, max_length=args.max_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = ResNetGRUAttention(num_classes=args.num_classes, hidden_size=args.hidden_size, max_length=args.max_length)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = FocalLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
        "\n",
        "    best_cer = float('inf')\n",
        "    patience = 5\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(args.num_epochs):\n",
        "        train_loss = train_epoch(model, dataloader, criterion, optimizer, device)\n",
        "\n",
        "        val_cer = validate(model, dataloader, dataset.idx_to_char, device)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{args.num_epochs}], Loss: {train_loss:.4f}, Validation CER: {val_cer:.4f}')\n",
        "\n",
        "        scheduler.step(val_cer)\n",
        "\n",
        "        if val_cer < best_cer:\n",
        "            best_cer = val_cer\n",
        "            no_improve = 0\n",
        "            save_model(model, 'best_model.pth')\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve == patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    return model, dataset  # model ve dataset'i döndür\n",
        "class ResNetGRUAttention(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size, max_length):\n",
        "        super(ResNetGRUAttention, self).__init__()\n",
        "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, hidden_size)\n",
        "\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(hidden_size * 2, num_heads=8)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.resnet(x)\n",
        "        features = features.unsqueeze(1).repeat(1, self.max_length, 1)\n",
        "\n",
        "        gru_out, _ = self.gru(features)\n",
        "\n",
        "        attn_out, _ = self.attention(gru_out, gru_out, gru_out)\n",
        "\n",
        "        output = self.fc(attn_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def main():\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.image_dir = \"/content/veri\"\n",
        "            self.label_file = \"/content/veri/label.txt\"\n",
        "            self.batch_size = 32\n",
        "            self.learning_rate = 0.001\n",
        "            self.hidden_size = 256\n",
        "            self.num_epochs = 50\n",
        "            self.num_classes = 1000  # Toplam benzersiz karakter sayısına göre ayarlayın\n",
        "            self.max_length = 100  # Maksimum etiket uzunluğuna göre ayarlayın\n",
        "\n",
        "    args = Args()\n",
        "    print(f\"Image directory: {args.image_dir}\")\n",
        "    print(f\"Label file: {args.label_file}\")\n",
        "\n",
        "    # Modeli eğit\n",
        "    model, dataset = train(args)\n",
        "\n",
        "    # Tahmin için modeli kullan\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha_t = self.alpha[target]\n",
        "            focal_loss = alpha_t * focal_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(2)), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, idx_to_char, device):\n",
        "    model.eval()\n",
        "    total_cer = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "\n",
        "            for pred, label in zip(predicted, labels):\n",
        "                pred_text = ''.join([idx_to_char[idx.item()] for idx in pred if idx.item() in idx_to_char])\n",
        "                label_text = ''.join([idx_to_char[idx.item()] for idx in label if idx.item() in idx_to_char])\n",
        "                total_cer += editdistance.eval(pred_text, label_text) / len(label_text)\n",
        "\n",
        "    return total_cer / len(dataloader.dataset)\n",
        "    # Modeli eğit\n",
        "    model, dataset = train(args)\n",
        "\n",
        "    # Tahmin için modeli kullan\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB91G6zl-PL6",
        "outputId": "b930e98c-e9b7-481d-b2af-0d6c56d888f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image directory: /content/veri\n",
            "Label file: /content/veri/label.txt\n",
            "Using device: cuda\n",
            "Image file not found: /content/veri/label.png\n",
            "Image file not found: /content/veri/1.png\n",
            "Image file not found: /content/veri/3.png\n",
            "Image file not found: /content/veri/23.png\n",
            "Image file not found: /content/veri/24.png\n",
            "Image file not found: /content/veri/25.png\n",
            "Image file not found: /content/veri/26.png\n",
            "Image file not found: /content/veri/27.png\n",
            "Image file not found: /content/veri/28.png\n",
            "Image file not found: /content/veri/29.png\n",
            "Image file not found: /content/veri/30.png\n",
            "Image file not found: /content/veri/31.png\n",
            "Image file not found: /content/veri/32.png\n",
            "Image file not found: /content/veri/33.png\n",
            "Image file not found: /content/veri/34.png\n",
            "Ignoring invalid line: 132.png iyi insanı secdelerden değil\n",
            "Ignoring invalid line: 162.png şüphesiz ki allah adaleti iyiliği ve akrabaya yardım etmeyi emir eder\n",
            "Ignoring invalid line: 174.png bizleri üç aylara kavuşturan rabime şükürler olsun\n",
            "Ignoring invalid line: 191.png emanete ihanet etmemesinden\n",
            "Ignoring invalid line: 238.png sen ne dersen de yine kadındır deliyi de adam eden\n",
            "Ignoring invalid line: 240.png sahibi hürmetine kulu incitme gönül\n",
            "Image file not found: /content/veri/250.png\n",
            "Loaded 228 valid image-label pairs\n",
            "Epoch [1/50], Loss: 3.7438, Validation CER: 0.9441\n",
            "Model saved to best_model.pth\n",
            "Epoch [2/50], Loss: 1.8919, Validation CER: 0.3874\n",
            "Model saved to best_model.pth\n",
            "Epoch [3/50], Loss: 1.4845, Validation CER: 0.3874\n",
            "Epoch [4/50], Loss: 1.4828, Validation CER: 0.4599\n",
            "Epoch [5/50], Loss: 1.4661, Validation CER: 0.3878\n",
            "Epoch [6/50], Loss: 1.3711, Validation CER: 0.3863\n",
            "Model saved to best_model.pth\n",
            "Epoch [7/50], Loss: 1.3739, Validation CER: 0.3842\n",
            "Model saved to best_model.pth\n",
            "Epoch [8/50], Loss: 1.3659, Validation CER: 0.3853\n",
            "Epoch [9/50], Loss: 1.3158, Validation CER: 0.3837\n",
            "Model saved to best_model.pth\n",
            "Epoch [10/50], Loss: 1.2926, Validation CER: 0.3863\n",
            "Epoch [11/50], Loss: 1.3695, Validation CER: 0.3875\n",
            "Epoch [12/50], Loss: 1.3086, Validation CER: 0.3857\n",
            "Epoch [13/50], Loss: 1.4016, Validation CER: 0.3843\n",
            "Epoch [14/50], Loss: 1.3352, Validation CER: 0.3814\n",
            "Model saved to best_model.pth\n",
            "Epoch [15/50], Loss: 1.3088, Validation CER: 0.3802\n",
            "Model saved to best_model.pth\n",
            "Epoch [16/50], Loss: 1.3023, Validation CER: 0.3828\n",
            "Epoch [17/50], Loss: 1.3451, Validation CER: 0.3803\n",
            "Epoch [18/50], Loss: 1.2760, Validation CER: 0.3801\n",
            "Model saved to best_model.pth\n",
            "Epoch [19/50], Loss: 1.2924, Validation CER: 0.3808\n",
            "Epoch [20/50], Loss: 1.2811, Validation CER: 0.3802\n",
            "Epoch [21/50], Loss: 1.2889, Validation CER: 0.3804\n",
            "Epoch [22/50], Loss: 1.3294, Validation CER: 0.3803\n",
            "Epoch [23/50], Loss: 1.2553, Validation CER: 0.3804\n",
            "Early stopping!\n"
          ]
        }
      ]
    }
  ]
}